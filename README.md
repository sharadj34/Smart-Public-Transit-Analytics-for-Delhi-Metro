## Smart Public Transit Analytics for Delhi Metro

Actionable analytics for Delhi Metro using open CSV datasets, SQL, and PostgreSQL. Includes reproducible scripts to create the database, load data, run analyses, and export results to the `outputs` folder.

### Features
- PostgreSQL-first workflow with portable SQL queries
- One-command runner to execute every analysis and save results
- Clean project layout with datasets, queries, and generated visualizations

## Project Structure
```
Datasets/                        # CSV inputs
postgres/                        # Postgres schema, loaders, and queries
  ├─ schema_postgres.sql         # Tables for stations, passengers, trips, revenue
  ├─ adjust_schema_postgres.sql  # Align schema with CSV headers/types
  ├─ load_csv_postgres.sql       # \copy commands to load CSVs
  ├─ queries_postgres.sql        # All analysis queries (Postgres-ready)
  ├─ run_all.ps1                 # Optional end-to-end runner
sql/                             # Source queries (DB-agnostic where possible)
scripts/
  └─ run_all_sql_to_outputs_postgres.ps1  # Runs each sql/*.sql → outputs/*.txt
outputs/                         # Auto-generated query outputs (created at runtime)
visualizations/                  # Existing charts/images
```

## Prerequisites
- PostgreSQL 14+ and `psql` in PATH
- PowerShell (Windows; script tested with PowerShell 5+/Core)

 

## Quickstart (PostgreSQL)
1) Set connection variables (adjust as needed):
```powershell
$env:PGHOST = "localhost"
$env:PGPORT = "5432"
$env:PGUSER = "postgres"
$env:PGPASSWORD = "<your_password>"
```

2) Create database and tables, align schema, load CSVs:
```powershell
psql -v ON_ERROR_STOP=1 -c "DROP DATABASE IF EXISTS delhi_metro; CREATE DATABASE delhi_metro;"
psql -v ON_ERROR_STOP=1 -d delhi_metro -f "postgres/schema_postgres.sql"
psql -v ON_ERROR_STOP=1 -d delhi_metro -f "postgres/adjust_schema_postgres.sql"
psql -v ON_ERROR_STOP=1 -d delhi_metro -f "postgres/load_csv_postgres.sql"
```

3) Run all analysis queries and save outputs:
```powershell
psql -v ON_ERROR_STOP=1 -d delhi_metro -P pager=off -f "postgres/queries_postgres.sql" -o "postgres/output_all.txt"
```

4) Or generate one output file per SQL in `sql/`:
```powershell
powershell -NoProfile -ExecutionPolicy Bypass -File ".\scripts\run_all_sql_to_outputs_postgres.ps1" -DbName "delhi_metro"
```
This creates text outputs for each query in the `outputs` folder, e.g. `outputs/01_basic_analysis.txt`.

## Datasets
- `Datasets/DelhiMetro_Stations.csv` (station_id, station_name, line_name, zone)
- `Datasets/DelhiMetro_Passengers.csv` (passenger_id, age, gender, card_type)
- `Datasets/DelhiMetro_Trips.csv` (trip_id, passenger_id, entry_station, exit_station, trip_datetime, fare)
- `Datasets/DelhiMetro_Revenue.csv` (revenue_id, station_id, revenue_date, total_revenue)

## Analyses Included
- Busiest stations by trips
- Peak travel hours
- Daily revenue trends and line-wise revenue contribution
- Passenger trends (by card type, gender, frequent travelers)
- Congestion (entry vs exit imbalance)
- Efficiency index (revenue per trip per station)

All are implemented in `postgres/queries_postgres.sql` and as individual files under `sql/`.

 

## Outputs and Visualizations
- Text outputs: `outputs/*.txt` (created by the Postgres runner)
- Aggregated output: `postgres/output_all.txt`
- Existing charts are under `visualizations/` (e.g., busiest stations, trips by hour, etc.)

## Jupyter Notebook Workflow
The repo includes a notebook for exploration and chart generation (`notebooks/metro_analysis_autosave.ipynb`). It reads CSVs with pandas and saves plots into `visualizations/`.

### Environment setup
Create a virtual environment and install dependencies:
```powershell
python -m venv .venv
. .venv/Scripts/Activate.ps1
pip install pandas matplotlib seaborn jupyter sqlalchemy psycopg2-binary
```

### Running the notebook
```powershell
jupyter notebook .\notebooks\metro_analysis_autosave.ipynb
```

The notebook will:
- Load `Datasets/*.csv` with pandas
- Generate and save plots to `visualizations/` via a helper that auto-creates the folder

To connect the notebook to PostgreSQL instead of SQLite, set env vars and use SQLAlchemy:
```python
import os
from sqlalchemy import create_engine

pg_url = f"postgresql+psycopg2://{os.environ['PGUSER']}:{os.environ['PGPASSWORD']}@{os.environ['PGHOST']}:{os.environ['PGPORT']}/delhi_metro"
engine = create_engine(pg_url)
df = pandas.read_sql("SELECT COUNT(*) FROM trips", engine)
```

### Visualizations generated by the notebook
The notebook currently generates and saves:
- `visualizations/delhi_metro_daily_revenue_trends.png`
- `visualizations/top_10_busiest_entry_stations.png`
- `visualizations/passenger_gender_distribution.png`
- `visualizations/passenger_gender_distribution_1.png`
- `visualizations/trips_by_hour_of_day.png`
- `visualizations/origin_destination_heatmap_trips_flow.png`
- `visualizations/zone_wise_trip_distribution.png`
- `visualizations/zone_wise_revenue_contribution.png`

Re-run the notebook to refresh these images after data changes.

## Maintenance
- If datasets change: re-run the load step and the outputs script.
- If you add new SQL files to `sql/`, rerun the outputs script to generate new results.

## License
Sharad Jha